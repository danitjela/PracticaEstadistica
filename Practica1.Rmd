---
title: 'Práctica Estadística Descriptiva- GRUPO 9'
author: "RAÚL BENÍTEZ TIBURÓN, ESTER DÍAZ MONZONIS, AROA QUIROGA MARTÍNEZ, SANDRA SÁEZ PIÑA, DANIELA TOCINO JIMÉNEZ"
date: "Estadística - Curso 2024/2025"

output: 
  bookdown::html_document2:
    toc: yes
    toc_float: yes
  # bookdown::pdf_document2:
  #   includes:
  #     in_header: preamble.tex
---

```{r setup, include=FALSE, purl=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = TRUE, 
                      warning = FALSE,
                      message = FALSE,
                      error = TRUE)
``` 

# Introducción y definición de objetivos

El archivo Steam_2024_bestRevenue_1500.csv contiene una recopilación de datos de los mejores 1500 juegos, según diferentes variables, lanzados en Steam desde el 1 de enero de 2024, hasta el 9 de septiembre de 2024.

El principal objetivo de nuestro estudio estadístico es dar respuesta a una serie de preguntas, y conocer las distintas relaciones existentes entre la recaudación de varios videojuegos y las diferentes variables que puedan llegar a afectar a este indicador.

En cuanto a las preguntas, hemos planteado las siguientes:

`- ¿Los indies recaudan más dinero que los AA o AAA?`

`- ¿Los juegos que venden más copias recaudan más dinero?`

`- ¿Los juegos más caros recaudan más dinero que los baratos?`

`- ¿Los juegos más largos recaudan más dinero que los que son más cortos?`

`- ¿Los juegos con buenas valoraciones recaudan más que los que tienen peor puntuación?`

También cabe mencionar que, el estudio de la variable "reviewScore" nos ha llamado la atención al observar que gran cantidad de los juegos tienen una valoración de 0, lo cual puede significar que, a la hora de cargar los datos en la página del dataset, ha podido haber algún error.


# Importación de datos y carga de paquetes

Para comenzar nuestro análisis descriptivo, necesitamos cargar una serie de paquetes en R que facilitarán la manipulación de datos, la creación de gráficos, y la presentación de tablas. Estos paquetes incluyen 'tidyverse' para la manipulación de datos, 'summarytools' y 'corrplot' para análisis estadísticos, 'GGally' para gráficos de pares, 'gt' y 'flextable' para tablas avanzadas, 'knitr' para la integración de R con documentos de texto, 'dplyr' para la modificación de los datos en un data frame.

```{r}

library(tidyverse)
library(summarytools)
library(GGally)
library(gt)
library(flextable)
library(knitr) 
library(corrplot)
library(dplyr)
library(kableExtra)

```

A continuación, leeremos y almacenaremos los datos de nuestro estudio. Para ello, utilizamos el método read.csv(), al cual le introducimos como parámetros: la ruta relativa del archivo, se especifica que el archivo tiene una fila de encabezados para los nombres de las variables y el tipo de separación de los elementos en el archivo (en nuestro caso commas). El resultado de esta operación se almacena en la variable lab con la que trabajaremos durante el análisis.

```{r, results = 'hide'}

  lab <- read.csv("DATA/Steam_2024_bestRevenue_1500.csv", header = TRUE, sep = ",")
  lab |>
  mutate(releaseDate = as.Date(releaseDate))
  
```


# Diccionario de datos

En esta sección, presentamos un diccionario de datos que detalla cada variable contenida en nuestro conjunto de datos 'Steam_2024_bestRevenue_1500.csv'. Las variables abarcan desde el nombre de cada uno de los juegos, hasta los desarrolladores de cada uno de ellos.


* `name`: Nombre del juego.
      - Variable tipo: Identificador.
      
* `releaseDate`: Fecha de salida del juego ("DD/MM/AAAA").
      - Variable tipo:Marca de tiempo.
      
* `copiesSold`: Número de copias vendidas (Unidades).
      - Variable tipo: Cuantitativa / Discreta.
      
* `price`: Precio original del juego de salida ($).
      - Variable tipo: Numérica / Discreta
      
* `revenue`: Cantidad de dinero recaudada por el juego ($).
      - Variable tipo: Cuantitativa / Discreta.
      
* `avgPlaytime`: Tiempo medio que los jugadores han empleado en el juego (Horas).
      - Variable tipo: Cuantitativa / Continua.
      
* `reviewScore`: Puntuación del juego en Steam (0-100).
      - Variable tipo: Cuantitativa / Discreta.
      
* `publisherClass`: Se definen las empresas como pequeña (indie), mediana (AA) o grande (AAA).
      - Variable tipo: Cualitativa / Multinivel.
      
* `publishers`: Empresa que se dedica a la publicación y marketing del juego.
      - Variable tipo: Identificador.
      
* `developers`: Empresa que se dedica a desarrollar y mantener el juego.
      - Variable tipo: Identificador.
      
* `steamId`: Id de steam
    - Variable tipo: Identificador.
      
    
# Muestra del dataset

Ahora, vamos a echar un vistazo a las primeras filas de nuestro dataset para obtener una comprensión inicial de los datos con los que estamos trabajando. 

```{r}

lab |>  
  slice_head(n=10) |> 
  gt()

```


# Análisis exploratorio de datos

## Datos univariantes

Comenzaremos nuestro análisis exploratorio de datos con un enfoque en los datos univariantes. Este enfoque nos permite entender la distribución y frecuencia de cada variable individualmente.

### Estudio de la variable RELEASEDATE

Para el estudio de la variable "releaseDate" crearemos un histograma, donde se agruparán las fechas de salida de cada videojuego en meses (desde enero hasta septiembre de este mismo año), y determinaremos cuál han sido los con más y menos lanzamientos.

```{r}
lab <- lab |> 
  mutate(releaseDate = as.Date(releaseDate, format = "%d-%m-%Y"))

lab <- lab |> 
  mutate(mes = format(releaseDate, "%m"), 
         mes = factor(mes, levels = sprintf("%02d", 1:12), labels = month.abb))

lanzamientos_mes <- lab |> 
  group_by(mes) |> 
  summarise(n_juegos = n(), .groups = 'drop')

  ggplot(lanzamientos_mes, aes(x = mes, y = n_juegos, fill = n_juegos)) +
    geom_bar(stat = "identity", color = "white", alpha = 0.8) +
    scale_fill_gradient2(low = "steelblue", mid = "skyblue", high = "red", midpoint = median(lanzamientos_mes$n_juegos)) +
    theme_bw() +
    labs(title = "Distribución de las fechas de lanzamiento",
         x = "Mes",
         y = "Nº Lanzamientos")
```

Según el gráfico podemos decir que los meses con más lanzamientos han sido: 1º -> Mayo, 2º -> Abril, 3º -> Marzo. Mientras tanto los meses con menos lanzamientos han sido: 1º -> Septiembre (ya que los datos van desde principios de este año hasta el 9 de septiembre), 2º -> Enero, 3º -> Agosto.


### Estudio de la variable COPIESSOLD

Primero, generamos una tabla de frecuencias y un histograma agrupando la variable 'copiesSold' por intervalos, esto nos ayuda a entender cuántas copias han sido vendidas de cada juego en toda nuestra muestra, si hay concentraciones de valores o si hay posibles outliers. Debido a que sus valores van desde 500 a 30.000.000 (ventas), asumimos que la distribución de dicha variable es dispersa y para solventarlo usaremos una escala logarítimica. 

```{r}

  lab$logCopiesSold <- log10(lab$copiesSold)

  lab$copiesSold_intervalos_log <- cut(lab$logCopiesSold, breaks = seq(floor(min(lab$logCopiesSold)), 
                                       ceiling(max(lab$logCopiesSold)), by = 0.5), include.lowest = TRUE, right = FALSE)

  lab$copiesSold_intervalos_etiquetas <- sapply(levels(lab$copiesSold_intervalos_log), function(x) {
    rango_log <- as.numeric(gsub("[^0-9.-]", "", unlist(strsplit(x, ","))))
    paste0("log10([", round(10^rango_log[1], 0), ", ", round(10^rango_log[2], 0), ")) = ", x ,"")
  })

  levels(lab$copiesSold_intervalos_log) <- lab$copiesSold_intervalos_etiquetas

  table(lab$copiesSold_intervalos_log) |> 
  
  as.data.frame() |> 

  mutate(freqRelativa = Freq / sum(Freq), freqAbsAcumulada = cumsum(Freq), freqRelAcumulada = cumsum(freqRelativa)) |>

  arrange(desc(Freq)) |>

  gt() |> 

  tab_header(
    title = md("**_Nº de copias vendidas_**")
  ) |>
  
  cols_label(Var1 = "\\(x_i\\)", Freq = "\\(n_i\\)", freqRelativa = "\\(f_i\\)", 
  freqAbsAcumulada = "\\(N_i\\)", freqRelAcumulada = "\\(F_i\\)") |>  
  
  fmt_number(3, decimals = 4) |> 

  tab_style(style = list (cell_text(weight = "bold")), locations = cells_column_labels()) |>

  tab_style(style = list(cell_text(weight = "bold")),  locations = cells_body(rows = c(1, 2))) 
  
```
Analizando las frecuencias podemos ver cómo casi la mitad del peso de las muestras se concentran en el intervalo [3.5,4.5), por lo que podemos asumir una desviación de las muestras hacia la izquierda y un sesgamiento general de los datos con muchos outliers (como ya se había presupuesto previamente). Como resultado, los valores más significativos son: entre las 3162 y 10000 ventas (31%) y entre las 10000 y 31623 ventas (27%).

```{r}

  ggplot(data.frame(x = lab$logCopiesSold), aes(x = x)) +
  geom_histogram(binwidth = 0.15, fill = "steelblue", color = "white", 
                 aes(y = ..count../sum(..count..), alpha = 0.6)) +
  stat_function(fun = dnorm, args = list(mean = mean(lab$logCopiesSold), 
                sd = sd(lab$logCopiesSold)), color = "red", size = 1.5) +
  theme_bw() +
  labs(title = "Distribución del Nº de copias vendidas",
       x = "Copias vendidas",
       y = "Frecuencia relativa") 

```

```{r}

  ggplot(data.frame(x = lab$logCopiesSold), aes(x = x)) +
  geom_histogram(binwidth = 0.15, fill = "steelblue", color = "white", 
                 aes(y = ..density..), alpha = 0.6) +
  stat_function(fun = dnorm, args = list(mean = mean(lab$logCopiesSold), 
                sd = sd(lab$logCopiesSold)), color = "red", size = 1.5) +
  theme_bw() +
  labs(title = "Distribución del Nº de copias vendidas",
       x = "Copias vendidas",
       y = "Frecuencia absoluta") 

```
Con el fin de facilitar el análisis del gráfico, a continuación se muestra un resumen numérico de la variable `copiesSold`:

```{r}
  lab|> select(copiesSold) |> descr() |> kable(digits = 2)
```

Según todos los datos y gráficos recabados en el análisis de esta variable, confirmamos que los datos las ventas están sesgados (Skewness = 18.58 // Kurtosis = 419.44 // IQR = 32951)), concretamente dicha distribución de ventas se encuentra sesgada hacia la derecha (cola derecha), esto se debe a que la media es más alta que la mediana (Mean = 141482.57 // Median = 11928.50) y al coeficiente de asimetría positivo (Skewness	= 18.58). Además, comparado con una distribución normal, podemos asegurar a través de su curtosis (Kurtosis =	419.44) que nuestra muestra es extremadamente más alta (idealmente Kurtosis =	3.00).

Tal y como estaba previsto, se trata de una distribución con pocos valores significativos y muchos outliers que alteran la medición de los datos y no aportan prácticamente ningún valor al análisis; aun así, resulta interesante saber que aproximadamente la mitad del peso de las ventas de la muestra se concentran entre las 3162 y las 31623 ventas y que el mínimo de ventas ha sido 593 y el máximo ha sido 30739148, lo cual énfatiza el amplio rango de datos de esta variable.

### Estudio de la variable PRICE

Para el estudio de 'price', crearemos una tabla de frecuencias y un histograma, proporcionando una visión clara de cuánto cuesta una copia de cada juego para así, descartar posibles outliers y hacer un análisis más preciso.

```{r}

  table(lab$price) |> 

  as.data.frame() |> 

  mutate(freqRelativa = Freq / sum(Freq), freqAbsAcumulada = cumsum(Freq), freqRelAcumulada = cumsum(freqRelativa)) |>

  arrange(desc(Freq)) |>

  gt() |> 

  tab_header(
    title = md("**_Precio por copia ($)_**")
  ) |>

  cols_label(Var1 = "\\(x_i\\)", Freq = "\\(n_i\\)", freqRelativa = "\\(f_i\\)", 
  freqAbsAcumulada = "\\(N_i\\)", freqRelAcumulada = "\\(F_i\\)") |>  

  fmt_number(3, decimals = 4) |> 

  tab_style(style = list (cell_text(weight = "bold")), locations = cells_column_labels()) |>

  tab_style(style = list(
    cell_text(weight = "bold")),  
    locations = cells_body(rows = c(1, 2, 3, 4, 5)))

```

Realmente no hay un precio que destaque en la muestra, ya que se trata de valores muy dispersos. A pesar de ello, podemos destacar los 5 primeros valores con más frecuencia: 19.99$ (15%), 14.99$ (10%), 9.99$ (9%), 24.99$ (8%), 29.99$ (6%). 

```{r}

  ggplot(data.frame(x = lab$price), aes(x = x)) +
  geom_histogram(binwidth = 5, fill = "steelblue", color = "white", 
                 aes(y = ..count../sum(..count..), alpha = 0.6)) +
  stat_function(fun = dnorm, args = list(mean = mean(lab$price), 
                sd = sd(lab$price)), color = "red", size = 1.5) +
  theme_bw() +
  labs(title = "Distribución del precio por copia ($)",
       x = "Precio por copia",
       y = "Frecuencia relativa") 

```

```{r}

  ggplot(data.frame(x = lab$price), aes(x = x)) +
  geom_histogram(binwidth = 5, fill = "steelblue", color = "white", 
                 aes(y = ..density..), alpha = 0.6) +
  stat_function(fun = dnorm, args = list(mean = mean(lab$price), 
                sd = sd(lab$price)), color = "red", size = 1.5) +
  theme_bw() +
  labs(title = "Distribución del precio por copia ($)",
       x = "Precio por copia",
       y = "Frecuencia absoluta") 

```
Con el fin de facilitar el análisis del gráfico, a continuación se muestra un resumen numérico de la variable `price`:

```{r}
  lab|> select(price) |> descr() |> kable(digits = 2)
```

Según todos los datos y gráficos recabados en el análisis de esta variable, podemos concluir con que efectivamente los precios están muy dispersos alrededor de la media (Mean = 17.52 // Std. Dev = 12.65), concretamente dicha distribución de precios se encuentra sesgada hacia la derecha (cola derecha), esto se debe a que la media es más alta que la mediana (Mean = 17.52 // Median = 14.99) y al coeficiente de asimetría positivo (Skewness	= 1.57). Además, comparado con una distribución normal, podemos asegurar a través de su curtosis (Kurtosis =	3.84) que nuestra muestra es ligeramente más alta (idealmente Kurtosis =	3.00).

En definitiva, se trata de una distribución con pocos valores significativos y muchos outliers que alteran la medición de los datos y no aportan prácticamente ningún valor al análisis; a pesar de ello, cabe destacar los 5 primeros precios más frecuentes: 19.99$ (15%), 14.99$ (10%), 9.99$ (9%), 24.99$ (8%), 29.99$ (6%) y el precio mínimo: 0.00$ y máximo: 99.99$.


### Estudio de la variable REVENUE

Analizamos la variable "revenue" mediante un histograma de los beneficios que nos muestra la distribución de los beneficios que obtienen estos videojuegos.

```{r}
ggplot(lab, aes(x = revenue)) +

  geom_histogram(binwidth = 2000000, fill = "steelblue", color = "white") +

  labs(title = "Distribución de los Beneficios de los Videojuegos",

       x = "Beneficios ($)",
       y = "Frecuencia") +

  ylim(0, 200) +  # Establece el rango del eje Y de 0 a 500
  xlim(0, 50000000) +  # Limita el rango del eje X a 50 millones

  theme_minimal()

```
En este histograma vemos la alta concentración de videojuegos con bajos beneficios, son menos frecuentes los juegos que alcanzan beneficios entre los 10 y los 50 millones. La gráfica muestra una clara asimetría con la cantidad que ganan poco y los pocos que obtienen grandes ingresos.


### Estudio de la variable AVGPLAYTIME

A continuación, nos enfocamos en el tiempo que dedican las personas a cada uno de los juegos.
Una herramienta crucial para entender la distribución de nuestra variable cuantitativa es el histograma, ya que este gráfico nos permitirá observar la forma y la dispersión de los datos.

Vamos a utilizar el color 'steelblue' para las barras, lo que nos permite observar la distribución de la variable de manera sencilla.

```{r}

lab |> ggplot(aes(avgPlaytime)) +
  
  geom_histogram(
    
    fill = "steelblue", 
             
    col = "white",    
    breaks = seq(0,100, by = 10)
  ) +

  theme_bw() +
    labs(x = "Tiempo promedio de juego (horas)", y = "Frecuencia", title = "Distribución de tiempo de juego por   intervalos")

```

Si analizamos el histograma se puede observar que la mayoría de los jugadores dedican menos de 10 horas a los videojuegos. Hay una caída significativa en la frecuencia de jugadores que juegan más de 10 horas, lo que sugiere que pocos jugadores le dedican tanto tiempo al juego. Esto puede indicar que muchos juegos no requieren un compromiso prolongado, o que los jugadores tienden a jugar varios titulos en sesiones cortas de tiempo.


A continuación, se crea una tabla de con los juegos que tienen más horas promedio jugadas.

```{r}

top_ten_games <- lab %>%
  arrange(desc(avgPlaytime)) %>%  
  slice(1:10) %>%                  
  select(name, avgPlaytime) %>%    
  mutate(avgPlaytime = round(avgPlaytime, 2))  


top_ten_games |> 
  kable(caption = "Los 10 juegos con más horas de juego") |> 
  kable_styling(full_width = F)

```

### Estudio de la variable REVIEWSCORE

```{r}

num_respuestas_SCORE <- length(unique(lab$reviewScore))
cat("La cantidad de respuestas obtenidas es:", num_respuestas_SCORE)

```

Esta variable la estudiaremos para poder ver más adelante si se puede relacionar con la recaudación de los juegos. A continuación, con el siguiente código esudiaremos las frecuencias relativas y absolutas por rangos debido a la cantida de valores que hay.

```{r}

max_val <- max(lab$reviewScore)
max_val <- min(max_val, 100)
lab$reviewScore_rango <- cut(lab$reviewScore, 
                             breaks = c(0, 1, seq(5, max_val, by = 5)),
                             right = FALSE, 
                             include.lowest = TRUE,
                             labels = c("0", paste(seq(1, max_val-4, by = 5), seq(5, max_val, by = 5)-1, sep = "-")))
tabla_frecuencias_score <- lab |>
  count(reviewScore_rango) |>
  mutate(f = n / nrow(lab), N = cumsum(n), F = cumsum(f))
tabla_frecuencias_ordenada4 <- tabla_frecuencias_score |> 
  arrange(desc(n))
tabla_frecuencias_ordenada4$F <- cumsum(tabla_frecuencias_ordenada4$f)
tabla_frecuencias_ordenada4$N <- cumsum(tabla_frecuencias_ordenada4$n)
print(tabla_frecuencias_ordenada4)

```

Como se puede observar hay bastantes valores de 0 que como hemos comentado debe tratarse de un error. Por ello, lo hemos sacado fuera de los rangos y para continuar con el estudio, eliminaremos ese valor para poder realizarlo de una manera más fiel a la realidad. Para esto, utilizaremos el siguiente código.

```{r}

labNo0 <- subset(lab, reviewScore != 0)

```

A continuación, realizaremos un histograma mediante el siguiente código.

```{r}

datos <- labNo0$reviewScore 
ggplot(data.frame(x = datos), aes(x = x)) +
  geom_histogram(binwidth = 1, fill = "steelblue", color = "white", 
                 aes(y = ..density..), alpha = 0.6) +
  stat_function(fun = dnorm, args = list(mean = mean(datos), 
                                         sd = sd(datos)), color = "red", size = 1.5) +
  labs(title = "Distribución de las puntuaciones",
       x = "Puntuación",
       y = "Frecuencia") 

```

Además de este histograma, debemos realizar un resumen númerico a partir del cual podremos comprender los gráficos con facilidad.

```{r}

labNo0 |> select(reviewScore) |> descr() |> kable(digits = 2)

```

Podemos observar como las puntuaciones por lo general, son extremadamente altas. Esto se puede deber a que el dataset es sobre los 1500 mejores juegos según su recaudación, por lo que podemos entender que son buenos juegos. Además, estas valoraciones son puestas por los usuarios y no por críticos profesionales.

### Estudio de la variable PUBLISHERCLASS

Ahora analizaremos la variable "publisherclass". En esta variable, veremos las diferencias entre los tipos de empresas que hay y como se distribuyen. A continuación, con el siguiente código estudiaremos las frecuencias relativas y absolutas.

```{r}

tabla_frecuencias_publisherclass <- lab |>
  count(publisherClass) |>
  mutate(f = n / nrow(lab)) 

print(tabla_frecuencias_publisherclass)

```

Para observar estos datos con mayor claridad, utilizaremos una gráfica en función de la frecuencia relativa con el siguiente código.

```{r}

lab  |> group_by(publisherClass) |>  summarise(Frequency = n()) |> mutate(Frequency = Frequency/sum(Frequency), publisherClass = reorder(publisherClass, -Frequency)) |> ggplot(aes(x = publisherClass, y = Frequency)) + geom_bar(stat = "identity", aes(fill = Frequency)) + scale_y_continuous(labels = scales::percent) + theme_bw() + labs(title = "Gráfico de barras con frecuencia relativa de PUBLISHERCLASS", x = "Tipo de empresa", y = "Frecuencia relativa") + scale_fill_gradient("Frecuencia", low = "steelblue", high = "#CB0017") + theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Como podemos observar, la mayor parte de los valores obtenidos, son del tipo "Indie", que es como se denota la empresa pequeña, ocupando el 87% de los casos. Además, se puede observar un caso asilado, "Hobbyist", que es como se denota a un particular que hace el juego por afición.  


### Estudio de la variable PUBLISHERS

A continuación analizamos la variable "publishers" mediante un gráfico de barras que muestra la frecuencia de los 10 publishers más representativos según la cantidad de juegos publicados en Steam. Primero contamos los videojuegos por publisher y seleccionamos los 10 más comunes y luego creamos en gráfico de barras con esos datos.

```{r}

top_publishers <- lab %>%

  count(publishers) %>%

  top_n(10, n)  

ggplot(top_publishers, aes(x = reorder(publishers, n), y = n)) +

  geom_bar(stat = "identity", fill = "steelblue", color = "white") +

  labs(title = "Top 10 Publishers por Número de Videojuegos",

       x = "Publisher",
       y = "Número de Videojuegos") +

  theme_minimal() +
  coord_flip()  

```

En esta gráfica podemos ver los 10 publishers que tienen una influencia notable en Steam debido al volumen de títulos publicados.

### Estudio de la variable DEVELOPERS


Ahora vamos a analizar la variable "developers". 
Realmente para nuestro estudio no es de gran importancia, aun así, crearemos una tabla de frecuencia para saber cuales son los 10 desarrolladores más frecuentes, ya que, puede ser curioso saberlo. 

```{r}

table(lab$developers) |> 
  as.data.frame() |> 
  
  arrange(desc(Freq)) |> 
  slice(1:10) |> 
  mutate(F = cumsum(Freq)/sum(Freq)) |> 
  gt() |>  
  
  tab_header(
    title = md("**_Los 10 desarrolladores más frecuentes_**")
  ) |>
  
  cols_label(Var1 = "Desarrollador",  
             Freq = "Frecuencia",  
             F = "Frecuencia relativa acumulada") |>  
  
  fmt_number(3, decimals = 4) |> 
  tab_style(style = list(
    cell_text(weight = "bold")),  
    locations = cells_column_labels())

```

En la tabla podemos observar claramente los 10 desarrolladores más frecuentes entre los 1407 desarrolladores únicos.
Como ya hemos mencionado, esta variable no será de gran importancia a la hora de responder a nuestras preguntas y de cumplir nuestro objetivo final de la práctica.


## Datos bivariantes

### Estudio de las variables COPIESSOLD Y REVENUE

Antes de comenzar con el analisis bivariante de "copiesSold" y "revenue" vamos a hacer un resumen de ambas variables para saber la cantidad de copias vendidas y la recaudación promedio, los valores mínimos y máximos, o los valores faltantes en caso de que los haya. Esto se hace con el objetivo de conocer mejor sus valores y distribución.

```{r}

sum(is.na(lab$copiesSold))
sum(is.na(lab$revenue))

summary(lab$copiesSold)
summary(lab$revenue)
```

A continuación, vamos a realizar un gráfico de dispersión para así saber si hay una relación visual en ambas variables.
```{r}
ggplot(lab, aes(x = copiesSold, y = revenue)) +
  geom_jitter(color = "blue", size = 1, width = 0.2, height = 0, alpha = 0.7) +
  scale_y_continuous(labels = function(x) {
    ifelse(x >= 1e6, 
           paste0(format(x / 1e6, big.mark = ",", digits = 2), " M"),
           ifelse(x >= 1e3, 
                  paste0(format(x / 1e3, big.mark = ",", digits = 2), " K"), 
                  format(x)))
  }) +
  scale_x_continuous(labels = function(x) {
    ifelse(x >= 1e6, 
           paste0(format(x / 1e6, big.mark = ",", digits = 2), " M"),
           ifelse(x >= 1e3, 
                  paste0(format(x / 1e3, big.mark = ",", digits = 2), " K"), 
                  format(x)))
  }) +
  labs(title = "Relación entre Copias Vendidas y Recaudación",
       x = "Copias Vendidas",
       y = "Recaudación (en logaritmos)") +
  theme_minimal()
```

Tras esto, calcularemos la correlación entre "copiesSold" y "revenue" para determinar si su relación es fuerte o débil, y representaremos el resultado en forma de tabla.

```{r}
correlacion <- cor(lab$copiesSold, lab$revenue, use = "complete.obs")

correlacion_df <- data.frame(Variable1 = "copiesSold", 
                             Variable2 = "revenue", 
                             Correlacion = round(correlacion, 2))
correlacion_df %>%
  kable(caption = "Correlación entre las copias vendidas y la recaudación") %>%
  kable_styling(full_width = F)
```

El resultado de la correlación es de 0.63, lo que indica una relación positiva significativa, aunque no muy fuerte. 

Por último, se realizará un modelo de regresión lineal para analizar cómo cambia la recaudación en función de las copias vendidas.

```{r}

modelo <- lm(revenue ~ copiesSold, data = lab)

summary(modelo)

ggplot(lab, aes(x = copiesSold, y = revenue)) +
  geom_point(size = 2, alpha = 0.7) +  
  geom_smooth(method = "lm", color = "blue") +
  scale_x_continuous(labels = function(x) {
    ifelse(x >= 1e6, 
           paste0(format(x / 1e6, big.mark = ",", digits = 2), " M"),
           ifelse(x >= 1e3, 
                  paste0(format(x / 1e3, big.mark = ",", digits = 2), " K"), 
                  format(x)))
  }) +
  scale_y_continuous(labels = function(x) {
    ifelse(x >= 1e6, 
           paste0(format(x / 1e6, big.mark = ",", digits = 2), " M"),
           ifelse(x >= 1e3, 
                  paste0(format(x / 1e3, big.mark = ",", digits = 2), " K"), 
                  format(x)))
  }) +
  labs(title = "Regresión Lineal: Copias Vendidas vs Recaudación",
       x = "Copias Vendidas",
       y = "Recaudación (en miles de dólares)") +
  theme_minimal()
```

Como se puede observar, las variables "copiesSold" y "revenue" se relacionan cuando el número de copias vendidas es inferior a 5 millones. La recaudación depende, aunque no exclusivamente, de las copias vendidas, pero a medida que se venden más copias, la relación entre ambas variables se vuelve más débil.

### Estudio de las variables PRICE Y REVENUE
En primer lugar, con el objetivo de comprender cómo varían los beneficios en cuanto al precio de los juegos de nuestra muestra y distinguir si hay una correlación entre dichas características, vamos a realizar una tabla de contingencia para ambas variables, donde podremos observar la frecuencia absoluta y los porcentajes (frec. relativa) de una variable frente a la otra.

```{r}

  lab$logrevenue <- log10(lab$revenue)

  lab$revenue_cat_log <- cut(lab$logrevenue, breaks = seq(floor(min(lab$logrevenue)), 
                                       ceiling(max(lab$logrevenue)), by = 0.5), include.lowest = TRUE, right = FALSE)

  lab$revenue_intervalos_etiquetas <- sapply(levels(lab$revenue_cat_log), function(x) {
    rango_log <- as.numeric(gsub("[^0-9.-]", "", unlist(strsplit(x, ","))))
    paste0("log10([", round(10^rango_log[1], 0), ", ", round(10^rango_log[2], 0), ")) = ", x ,"")
  })

  levels(lab$revenue_cat_log) <- lab$revenue_intervalos_etiquetas
  
  price_cat <- cut(lab$price, breaks = seq(0, 100, by = 5), right = FALSE)  # 4 intervalos para y

  tabla_abs_priceRevenue <- table(lab$revenue_cat_log, price_cat)

  tabla_rel_priceRevenue <- prop.table(tabla_abs_priceRevenue) * 100
  
  tabla_contingencia_priceRevenue <- matrix(paste(tabla_abs_priceRevenue, "(", round(tabla_rel_priceRevenue, 2), "%)", sep = ""),
                            nrow = nrow(tabla_abs_priceRevenue), ncol = ncol(tabla_abs_priceRevenue))

  rownames(tabla_contingencia_priceRevenue) <- rownames(tabla_abs_priceRevenue)
  colnames(tabla_contingencia_priceRevenue) <- colnames(tabla_abs_priceRevenue)

  kable(tabla_contingencia_priceRevenue)
  
```

Ante los datos de esta tabla, podemos obtener una primera conclusión, no existe una fuerte relación (aparentemente) entre los beneficios obtenidos y el precio impuesto para el juego. Sin embargo podemos apreciar una concentración de frecuencias entre los primeros intervalos de ambas variables, lo que indica que a priori, puede haber una correlación (ciertamente no muy elevada) entre la obtención de menores beneficios e imponer un precio bajo (entre 0$ y 20$).

Curiosamente, ni imponer un precio más elevado asegura mayores beneficios, ni imponer un precio más bajo (incluso 0$) implica beneficios menores.


A continuación, realizaremos dos gráficas de dispersión, la primera estará basada en el ajuste de regresión más adecuado posible (mediante la función geom_smooth) y la segunda, continuando así el estudio de correlación, consistirá en un ajuste de regresión lineal.

```{r}
lab |>
  ggplot(aes(x = logrevenue, y = price)) +
  geom_point() +
  geom_smooth() +
  labs(title = "Gráfico de dispersión con ajuste de regresión")
```
```{r}
lab |> 
  ggplot(aes(x = logrevenue, y = price)) +
  geom_point() +
  geom_smooth(method = lm, se = FALSE) +
  labs(title = "Gráfico de dispersión con ajuste de regresión lineal")
```

En cuanto a los distintos ajustes de la recta, no hay una diferencia significativa entre un método y el otro. Según ambas rectas, a simple vista podemos denotar un aumento positivo y progresivo de los beneficios en cuanto al aumento del precio.


Para mayor comprensión de ambos gráficos, mostramos el siguiente resumen numérico:

```{r}
  summary(lab |> select(logrevenue, price) |> with(lm(logrevenue ~ price)))
```

Aproximadamente, a través de los valores del resumen (Estimate Std (logrevenue) = 4.82304 // Estimate Std (price) = 0.02350) la recta en términos logarítmicos de nuestro modelo ajustado es la siguiente:

`logrevenue = 0.02 × price + 4.82`


Despejando el logaritmo, para una mejor interpretación, nos queda la siguiente recta:

`revenue = (e ^ (0.02 * price)) × 123.97` 

ó

`revenue = (1.02 ^ price) × 123.97`

Lo que indica una función exponencial creciente, donde a más aumentan los precios, los ingresos aumentan drásticamente; asímismo, con precios inferiores los ingresos incrementan muy lentamente. Además dicha función nos indica el valor esperado de beneficios cuando el precio es 0$ (valor totalmente válido en este estudio) que sería 123.97$.

Analizando más valores del resumen tenemos que el modelo es altamente significativo estadísticamente para el estudio de la muestra debido a que el valor F-statistic (281.7) es alto  y el p-value (< 2.2e-16) es bajo; además, se trata de un modelo altamente preciso ya que el valor del Residual standard error(0.6856) es muy cercano al valor ideal (entorno a 0). 

Por último, en cuanto a correlación de una variable con otra y predictibilidad del modelo creado, el valor R-squared (0.1583) indica que el 15.83% de la variabilidad en el logaritmo de los ingresos es explicada por el precio, el cual no es un porcentaje extremadamente alto. Esto sugiere que, si bien el precio afecta los ingresos, hay otros factores que también influyen en los ingresos que no están considerados en el modelo.


### Estudio de las variables AVGPLAYTIME Y REVENUE

Lo primero de todo, para saber si hay una relación fuerte entre el tiempo de juego de las personas (avgPlaytime)y la recaudación (revenue), vamos a utilizar una tabla de correlación.

```{r}

correlacion <- cor(lab$avgPlaytime, lab$revenue, use = "complete.obs")

correlacion_df <- data.frame(Variable1 = "avgPlaytime", 
                             Variable2 = "revenue", 
                             Correlacion = round(correlacion, 2))

correlacion_df %>%
  kable(caption = "Correlación entre el tiempo de juego medio y la recaudación") %>%
  kable_styling(full_width = F)

```


La tabla nos indica que la correlación entre ambas variables es de 0.08, es decir, una cifra cercana a 0. Esto nos dice que la relación entre ambas variables es muy débil o inexistente.

Los juegos con menos horas de juego pueden tener precios altos y al contrario, los juegos muy largos pueden atraer a jugadores que invierten más tiempo, pero sin tener que gastar más dinero.

A continuación, vamos a visualizar un gráfico de dispersión para poder ver la relación entre las variables de forma más clara y directa.Los puntos en el gráfico representan diferentes juegos, donde el eje "x" indica el tiempo promedio de juego, y el eje "y" representa la recaudación total obtenida por esos juegos.

Para obtener una mejor comprensión visual, se ha utilizado una escala logarítmica en el eje de recaudación debido a la amplia variación en las cifras de ingresos, que van desde valores muy bajos hasta cientos de millones. Esto nos permite que los datos sean más visibles y comparables.


```{r}
lab |> 
  ggplot(aes(x = avgPlaytime, y = revenue)) +
  
  geom_point(alpha = 0.7, color = "blue") +
  
  scale_y_log10() +
  
  xlim(0, 300) +

  labs(
    x = "Tiempo promedio de juego (horas)",
    y = "Recaudación (log scale)",
    title = "Relación entre Tiempo de Juego y Recaudación en Steam"
  ) +

  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
    axis.title.x = element_text(face = "bold"),
    axis.title.y = element_text(face = "bold")
  )

```
El gráfico muestra una concentración de juegos con un tiempo promedio bajo (cercano a 0 horas), y una gran dispersión en los valores de recaudación. Sin embargo, a pesar de que algunos juegos tienen tiempos de juego elevados (cercanos a las 300 horas), estos no necesariamente generan más ingresos que otros con menos tiempo jugado.

Los puntos se distribuyen en su mayoría en la esquina inferior izquierda (poco tiempo jugado y baja recaudación), pero no hay un patrón claro de que a mayor tiempo jugado haya una mayor recaudación. El hecho de que los puntos estén tan juntos en la parte baja también sugiere que la mayoría de los juegos tienen tiempos de juego relativamente cortos.


Por último, para determinar de forma definitiva si el tiempo promedio de juego es un buen predictor de ingresos, vamos a observar un modelo de regresión.

```{r}

modelo <- lm(lab$revenue ~ lab$avgPlaytime)
summary(modelo)  

```


El valor estimado de Intercept es 1,295,594, lo que significa que si el tiempo promedio de juego fuera 0, la recaudación promedio  sería de aproximadamente 1.3 millones de unidades monetarias.

La pendiente es 106,409, lo que significa que por cada hora adicional de tiempo promedio de juego, la recaudación promedio aumenta en 106,409 unidades monetarias.

El valor p para avgPlaytime es 0.0014, lo que indica que hay una relación significativa entre las variables con un nivel de confianza del 99.86% (muy significativo dado que p<0.01).

Esto significa que el efecto de avgPlaytime en revenue no es probablemente debido al azar.

El R^2 es 0.006794, lo que indica que el tiempo promedio de juego explica solo el 0.68% de la variabilidad en los ingresos. Esto es muy bajo, lo que sugiere que hay otros factores importantes que afectan la recaudación y que el tiempo de juego por sí solo no es un buen predictor.

A modo de conclusión podemos decir que el tiempo de juego influye de forma mínima en la recaudación.


### Estudio de las variables REVIEWSCORE Y REVENUE
A continuación vamos a analizar  si la puntuación asociada al videojuego afecta realmente a la recaudación. Para ello vamos a utilizar una gráfica de dispersión mediante la cual se verá la cantidad de videojuegos beneficiados por su puntuación.

```{r}

ggplot(data = lab, aes(x = reviewScore, y = revenue)) +

  geom_point(alpha = 0.5) +  

  labs(title = "Relación entre Puntuación y Recaudación",

       x = "Puntuación",
       y = "Recaudación") +

  theme_minimal()  # Usar un tema minimalista

```

Podemos apreciar cómo la mayoría de puntos se agrupan en la parte baja de la gráfica indicando que pese a  la puntuación alta que puedan tener, son pocos los que tienen recaudaciones altas. Lo que sí queda claro es que de los juegos con poca puntuación, ninguno sobresale. De todas formas comprobemos mediante un análisis de regresión para obtener un modelo cuantitativo.

```{r}

modelo <- lm(revenue ~ reviewScore, data = lab) 

summary(modelo)  

ggplot(data = lab, aes(x = reviewScore, y = revenue)) +

  geom_point(alpha = 0.5) +  

  geom_smooth(method = "lm", color = "blue") +  

  labs(title = "Relación entre Puntuación y Recaudación con Línea de Regresión",

       x = "Puntuación",
       y = "Recaudación") +

  theme_minimal()

```
Ahora podemos ver cómo la línea de regresión no cambia, se mantiene prácticamente horizontal. Esto deja claro que la recaudación que se obtiene no tiene mucho que ver con la puntuación  que tenga el videojuego, son escasos los que por tener una alta puntuación tienen tambien una alta recaudación.


### Estudio de las variables PUBLISHERCLASS Y REVENUE
En este apartado, queremos comprobar si el tipo de empresa afecta a la recaudación. Para ello, vamos a utilizar una gráfica box-plot y un gráfico de barras para que sea más visual.

```{r}

ggplot(lab, aes(x = publisherClass, y = revenue)) +
  geom_bar(stat = "summary", fun = "mean", fill = "steelblue") +  # Barras que muestran la media
  labs(title = "Recaudación por tipo de empresa",
       x = "Tipo de empresa",
       y = "Recaudación promedio") 

```

Como se puede observar, los juegos que menos recaudan, son los de empresa tipo "Indie", a pesar de que estos son la mayoría de los juegos que salen. A continuación, realizaremos una gráfica box-plot para analizar mejor los datos. Por otra parte, esta el tipo "Hobbyist", que no tiene sentido analizar debido a que es una única prueba, y el objetivo de ese tipo de empresa no es la recaudación de dinero.

```{r}

ggplot(lab, aes(x = publisherClass, y = revenue)) +
  geom_boxplot(fill = "lightblue", color = "darkblue") +
  labs(title = "Distribución de la Recaudación por Tipo de Empresa",
       x = "Tipo de Empresa",
       y = "Recaudación") +
  theme_minimal()

```
Debido a la disparidad entre las muestras que tenemos, necesitaremos aplicar una escala logarítmica para evitar la asimetría en dichas muestras ya que la distribuición es muy sesgada.

```{r}

ggplot(lab, aes(x = publisherClass, y = revenue)) +
  geom_boxplot(fill = "lightblue", color = "darkblue") +
  scale_y_log10() +  
  labs(title = "Distribución de la Recaudación por Tipo de Empresa (Escala Logarítmica)",
       x = "Tipo de Empresa",
       y = "Recaudación (log)") +
  theme_minimal()

```

Aquí podemos observar como sí hay juegos "Indie" con una recaudación superior a la media del resto de empresas, pero son casos aislados y la mayor parte de los "Indies", generan mucho menos que los juegos "AAA" y "AA". 


# Conclusiones 



## LINK AL REPOSITORIO DE GITHUB

1. **Repositorio de GitHub**: https://github.com/danitjela/PracticaEstadistica
